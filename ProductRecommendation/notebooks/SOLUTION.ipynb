{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from src.config import PATH, DATA_DIR, MODELS_DIR\n",
    "from src.data.create_dataset import Dataset, DatasetComplete\n",
    "from src.data.process_data import (save_processed_datasets_to_feather,\n",
    "                                   load_processed_datasets_from_feather)\n",
    "\n",
    "ZIP_PATH = DATA_DIR / 'purchaseprediction_fulldata.zip'\n",
    "INTERIM_DATA_DIR = DATA_DIR / 'interim'\n",
    "SEED = 42\n",
    "\n",
    "GPU_AVAILABLE = False\n",
    "LOAD_PRETRAINED = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unzip the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the zipped data \"purchaseprediction_fulldata.zip\" in the ProductPrediction/data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_fnames = [\n",
    "    'customers','labels_predict',\n",
    "    'labels_training','products',\n",
    "    'purchases','views']\n",
    "\n",
    "# check if file has already been unzipped\n",
    "if all(os.path.exists(DATA_DIR/'raw'/f'{fname}.txt') for fname in raw_fnames):\n",
    "    print('already unzipped')\n",
    "\n",
    "# extract zipfile to 'raw'\n",
    "else:\n",
    "    assert ZIP_PATH.exists()\n",
    "    # create directory if needed\n",
    "    if not os.path.exists(DATA_DIR/'raw'):\n",
    "        os.mkdir(DATA_DIR/'raw')\n",
    "    # extract\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATA_DIR / 'raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Save Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the raw datasets, imputing missing values and reducing data usage. Then save all to feather format for faster loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if feather files alreeady exist then skip\n",
    "if all(os.path.exists(INTERIM_DATA_DIR/f'{fname}.feather') for fname in raw_fnames):\n",
    "    print('Processed feathers already exist')\n",
    "\n",
    "# else create and save\n",
    "else:\n",
    "    save_processed_datasets_to_feather()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the final model we train on the full dataset without cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise dataset object\n",
    "data = Dataset(random_seed=SEED)\n",
    "\n",
    "train_fpath = DATA_DIR/'processed'/f'train.feather'\n",
    "valid_fpath = DATA_DIR/'processed'/f'valid.feather'\n",
    "\n",
    "# if data has already been processed, load from file\n",
    "if train_fpath.exists() & valid_fpath.exists():\n",
    "    data.load_datasets_from_file(train_fpath, valid_fpath)\n",
    "\n",
    "# otherwise create the datasets\n",
    "# Load the feather datasets and merge\n",
    "# compute engineered features and apply to both train and test sets\n",
    "else:\n",
    "    (customer_df,\n",
    "     product_df,\n",
    "     purchase_df,\n",
    "     views_df,\n",
    "     labels_training_df,\n",
    "     labels_testing_df) = load_processed_datasets_from_feather()\n",
    "    \n",
    "    data.create_train_valid_datasets(\n",
    "        labels_training_df, labels_testing_df,\n",
    "        customer_df, purchase_df, product_df, views_df)\n",
    "\n",
    "    # save the resulting datasets\n",
    "    data.save_datasets(train_fpath, valid_fpath)\n",
    "    \n",
    "    del (labels_training_df, labels_testing_df, customer_df, purchase_df, product_df, views_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To combat class imbalance we use Random Over Sampling on the training data. We do not need to standardise our data as XGBoost is tree based, so is not affected by feature scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PRETRAINED:\n",
    "    # for probability calibration\n",
    "    # % true class in original dataset (0.02)\n",
    "    true_ratio = data.train.purchased.mean()\n",
    "\n",
    "    # resample\n",
    "    sampler = RandomOverSampler(random_state=SEED)\n",
    "    data.train, _ = sampler.fit_resample(data.train, data.train.purchased)\n",
    "\n",
    "    # for probability calibration\n",
    "    # % true class in resampled dataset (0.5)\n",
    "    wrong_ratio = data.train.purchased.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and train a model using the best hyperparameters found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_PRETRAINED:\n",
    "    params = dict(\n",
    "        learning_rate=0.01,\n",
    "        n_estimators=2000,\n",
    "        sampling_method='gradient_based' if GPU_AVAILABLE else 'uniform',\n",
    "        eval_metric='logloss',\n",
    "        subsample=0.8,\n",
    "        tree_method='gpu_hist' if GPU_AVAILABLE else 'hist',\n",
    "        colsample_bynode=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        use_label_encoder=False,\n",
    "        reg_alpha=100,\n",
    "        max_depth=6)\n",
    "\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "\n",
    "    model.fit(\n",
    "        data.train.drop(columns=['purchased']),\n",
    "        data.train.purchased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally save this trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = MODELS_DIR/'trained_xgboost.joblib'\n",
    "# joblib.dump(model, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively load the already trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_PRETRAINED:\n",
    "    model = joblib.load(MODELS_DIR/'final_xgboost.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the fitted model to make predictions on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions for the test set probabilities\n",
    "y_pred = model.predict_proba(data.valid.drop(columns=['purchase_probability']))[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are predicting probabilities and not classes, we need to calibrate our model outputs. [Derivation of class adjustment formula](/ProductRecommendation/reports/Adjustment%20formula%20derivation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classadjust(condprobs,wrong_ratio,true_ratio):\n",
    "    \"\"\"Adjust predicted probabilities - calibration\"\"\"\n",
    "    a = condprobs/(wrong_ratio/true_ratio)\n",
    "    comp_cond = 1 - condprobs\n",
    "    comp_wrong = 1 - wrong_ratio\n",
    "    comp_true = 1 - true_ratio\n",
    "    b = comp_cond/(comp_wrong/comp_true)\n",
    "    return a/(a+b)\n",
    "\n",
    "if LOAD_PRETRAINED:\n",
    "    true_ratio = 0.020212026484729476\n",
    "    wrong_ratio = 0.5\n",
    "\n",
    "y_pred_adj = classadjust(y_pred, wrong_ratio, true_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our predictions, we can output our results to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_feather(DATA_DIR/'interim'/'labels_predict.feather')\n",
    "submission.purchase_probability = y_pred_adj\n",
    "\n",
    "submission.to_csv('SUBMISSION.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If doing cross validation for model comparison the data in each fold needs to be processed separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "n_folds = 3\n",
    "\n",
    "# create dataset - load preprocessed from files\n",
    "data = DatasetComplete(random_seed=SEED)\n",
    "data.load_datasets()\n",
    "data.assign_folds(load_from_path=DATA_DIR/'interim'/f'cv_folds_{n_folds}.npy')\n",
    "data.load_nfolds_from_files(n_folds=n_folds, save_filepath=DATA_DIR/'processed'/'Extra features')\n",
    "\n",
    "# resample all the folds individually - using sampler\n",
    "sampler = RandomOverSampler(random_state=SEED)\n",
    "for i in range(n_folds):\n",
    "    data.folds_data[i].train, _ = sampler.fit_resample(\n",
    "        data.folds_data[i].train, data.folds_data[i].train.purchased)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train a model on each fold - which can be accessed through <code>data.folds_data[i].train</code>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f116c7878aab50998026fc56fb7eb8f990dc6995904e4a3c551baa74ed33665d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('ASOSProductPrediction': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
